{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d5e4d2",
   "metadata": {},
   "source": [
    "t-inference: 5.1, 5.3, 5.5, 5.13, 5.17, 5.19, 5.21, 5.23, 5.27, 5.31, 5.35, 5.37\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af25d76",
   "metadata": {},
   "source": [
    "**5.1 Identify the critical t.** An independent random sample is selected from an <u>approximately normal population</u> with unknown standard deviation. Find the degrees of freedom and the critical t-value ($t^*$) for the given sample size and confidence level.  \n",
    " (a) n=6,CL=90%   \n",
    " (b) n=21,CL=98%   \n",
    " (c) n=29,CL=95%   \n",
    " (d) n=12,CL=99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaa25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.015048372669157\n",
      "2.527977002740546\n",
      "2.048407141795244\n",
      "3.1058065132211006\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.stats.t.ppf(0.95, 5))\n",
    "print(scipy.stats.t.ppf(0.99, 20))\n",
    "print(scipy.stats.t.ppf(0.975, 28))\n",
    "print(scipy.stats.t.ppf(0.995, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447ec1f",
   "metadata": {},
   "source": [
    "**5.3 Find the p-value, Part I.** An independent random sample is selected from an <u>approximately normal population</u> with an unknown standard deviation. Find the p-value for the given set of hypotheses and T test statistic. Also determine if the null hypothesis would be rejected at α = 0.05.  \n",
    "(a) HA :μ>μ0,n=11,T =1.91, reject null hypothesis.  \n",
    "(b) HA :μ<μ0,n=17,T =−3.45, reject null hypothesis.    \n",
    "(c) HA :μ̸=μ0,n=7,T =0.83, fail to reject null hypothesis.    \n",
    "(d) HA :μ>μ0,n=28,T =2.13, reject null hypothesis.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cf7862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04260243832257815\n",
      "0.0016467857018984567\n",
      "0.4383084015659864\n",
      "0.021217692960949996\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(1 - scipy.stats.t.cdf(1.91,10))\n",
    "print(scipy.stats.t.cdf(-3.45,16))\n",
    "print(2 * (1 - scipy.stats.t.cdf(0.83,6)))\n",
    "print(1 - scipy.stats.t.cdf(2.13,27))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be4e12",
   "metadata": {},
   "source": [
    "**5.4 Find the p-value, Part II.** An independent random sample is selected from <u>an approximately normal population</u> with an unknown standard deviation. Find the p-value for the given set of hypotheses and T test statistic. Also determine if the null hypothesis would be rejected at α = 0.01.  \n",
    "(a) HA :μ>0.5,n=26,T =2.485, reject null hypothesis.  \n",
    "(b) HA :μ<3,n=18,T =0.5, fail to reject null hypothesis.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facc49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010002400548908885\n",
      "0.6882573967171443\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(1 - scipy.stats.t.cdf(2.485,25))\n",
    "print(scipy.stats.t.cdf(0.5,17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26de763",
   "metadata": {},
   "source": [
    "**5.5 Working backwards, Part I.** A 95% confidence interval for a population mean, μ, is given as (18.985, 21.015). This confidence interval is based on a simple random sample of 36 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd31f9",
   "metadata": {},
   "source": [
    "**Ans:** we know lower bound = 18.985, upper bound = 21.015, n= 36, $t^* = 2.03$  \n",
    "$lower bound = \\bar{x} - t^* * \\frac{s}{\\sqrt{n}}$,  \n",
    "$upper bound = \\bar{x} + t^* * \\frac{s}{\\sqrt{n}}$    \n",
    "Hence,  \n",
    "$\\bar{x} = \\frac{lower bound + upper bound}{2}$    \n",
    "\n",
    "$s = lower bound - \\bar{x} + \\frac{t^*}{\\sqrt{n}}$.   \n",
    "plug the known into the formula,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d417a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0301079282503425"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "t_critical = scipy.stats.t.ppf(0.975, 35)\n",
    "t_critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5f8afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0, -0.6766486786249435]\n"
     ]
    }
   ],
   "source": [
    "x_bar = (18.985+21.015)/2\n",
    "s = 18.985-x_bar + t_critical/6\n",
    "print([x_bar,s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f29cd",
   "metadata": {},
   "source": [
    "**5.13 Car insurance savings.** A market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $100$. He wants to collect data such that he can get a margin of error of no more than $10 at a 95% confidence level. How large of a sample should he collect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a4b5c",
   "metadata": {},
   "source": [
    "**Ans:** $Margin of Error = 1.96 * 100/\\sqrt{n} = 10$  \n",
    "hence,  \n",
    "n = 385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866f4b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1600000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.power(1.96*100/10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140ff4f",
   "metadata": {},
   "source": [
    "**5.17 Paired or not, Part I?** In each of the following scenarios, determine if the data are paired.  \n",
    "(a) Compare pre- (beginning of semester) and post-test (end of semester) scores of students. **paired**  \n",
    "(b) Assess gender-related salary gap by comparing salaries of randomly sampled men and women. **not paired**.   \n",
    "(c) Compare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin E for the same group of patients. **paired**.   \n",
    "(d) Assess effectiveness of a diet regimen by comparing the before and after weights of subjects. **paired**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde851a3",
   "metadata": {},
   "source": [
    "**5.19 Global warming, Part I.** Is there strong evidence of global warming? Let’s consider a small scale example, comparing how temperatures have changed in the US from 1968 to 2008. The daily high temperature reading on January 1 was collected in 1968 and 2008 for 51 randomly selected locations in the continental US. Then the difference between the two readings (temperature in 2008 - temperature in 1968) was calculated for each of the 51 different locations. The average of these 51 values was 1.1 degrees with a standard deviation of 4.9 degrees. We are interested in determining whether these data provide strong evidence of temperature warming in the continental US.  \n",
    "(a) Is there a relationship between the observations collected in 1968 and 2008? Or are the observations in the two groups independent? Explain.  \n",
    "**Ans:** independent.  temperatures in one year do not impact temperatures in another year.  \n",
    "\n",
    "(b) Write hypotheses for this research in symbols and in words.  \n",
    "**Ans:**  \n",
    "$$H_0: \\mu_{2008} - \\mu_{1968} = 0$$.  no temperature warming in the continential US  \n",
    "$$H_A: \\mu_{2008} - \\mu_{1968} \\gt 0$$. temperature is warmer in 2008 compared to 1968 in the continential US  \n",
    "\n",
    "(c) Check the conditions required to complete this test.  \n",
    "**Ans:**  The 51 locations were randomly selected, so likely the observation in our sample is independent in both time points. sample size for both group is 51, so the sample size is considered large enough for CLT to be applied assuming the population distribution is not extremely skewed. If I have the dataset, I would run a histogram plot of both data set to check the shape and outliers. I would also check third party reseach in terms of temperature distribution in US continental to make sure the skewness assumption do not deviate too much.  \n",
    "\n",
    "(d) Calculate the test statistic and find the p-value.  \n",
    "**Ans:**  \n",
    "$$n = 51, x_{diff}  = 1.1, s_diff = 4.9, df = 50$$\n",
    "\n",
    "$$T = \\frac{x_{diff} - \\mu_0}{s_{diff}/\\sqrt{n}} = \\frac{1.1-0}{4.9/\\sqrt{51}} \\approx 1.6032$$\n",
    "\n",
    "$$ p-value \\approx 0.05760 \\gt 0.05$$\n",
    "\n",
    "(e) What do you conclude? Interpret your conclusion in context.  \n",
    "**Ans:**  \n",
    "Failed to reject null hypothesis. The evidence does not strong enough to support the claim that there is global warming in the continental US.  \n",
    "\n",
    "(f) What type of error might we have made? Explain in context what the error means.  \n",
    "**Ans:**  \n",
    "We may make a Type II error which means our sample data did not capture the global warming phenomenon when there is truely global warming in the continental US.  \n",
    "\n",
    "(g) Based on the results of this hypothesis test, would you expect a confidence interval for the average difference between the temperature measurements from 1968 and 2008 to include 0? Explain your reasoning.  \n",
    "**Ans:**  \n",
    "Yes. a confidence interval would include the null value when the hypothesis test fails to reject null hypothesis.  In this case, the null value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a2216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6031778104892114\n",
      "0.05759730789200801\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "t_score = (1.1-0)/(4.9/np.sqrt(51))\n",
    "print(t_score)\n",
    "print(1 - scipy.stats.t.cdf(t_score, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21961ad6",
   "metadata": {},
   "source": [
    "**5.21 Global warming, Part II.** We considered the differences between the temperature readings in January 1 of 1968 and 2008 at 51 locations in the continental US in Exercise 5.19. The mean and standard deviation of the reported differences are 1.1 degrees and 4.9 degrees.  \n",
    "(a) Calculate a 90% confidence interval for the average difference between the temperature measurements between 1968 and 2008.  \n",
    "**Ans:**   \n",
    "[-0.0499, 2.2499]  \n",
    "\n",
    "(b) Interpret this interval in context.  \n",
    "**Ans:**   \n",
    "we are 90% confidence that the average difference between 1968 and 2008 in the US continent is between [-0.0499, 2.2499] degrees. \n",
    "\n",
    "(c) Does the confidence interval provide convincing evidence that the temperature was higher in 2008 than in 1968 in the continental US? Explain.  \n",
    "**Ans:**   \n",
    "Because the confidence interval include 0, in the context of hypothesis test, we fail to reject the null hypothesis which claims there is no difference in tempature between 1968 and 2008 in contenental US.  Therefore, CI does NOT provide convincing evidence that the temperature was higher in 2008 than in 1968 in the continental US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ead22c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049900849999052754, 2.2499008499990527]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 51\n",
    "df = n - 1\n",
    "x_diff = 1.1\n",
    "s_diff = 4.9\n",
    "alpha = 0.1\n",
    "t_critical = scipy.stats.t.ppf(1-alpha/2,df)\n",
    "se_diff = s_diff / np.sqrt(n)\n",
    "print([x_diff-t_critical * se_diff, x_diff + t_critical * se_diff])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc4985",
   "metadata": {},
   "source": [
    "**5.23 Gifted children.** Researchers collected a simple random sample of 36 children who had been identified as gifted in a large city. The following histograms show the distributions of the IQ scores of mothers and fathers of these children. Also provided are some sample statistics.  \n",
    "(a) Are the IQs of mothers and the IQs of fathers in this data set related? Explain.  \n",
    "**Ans:**  \n",
    "children is randomly selected, when selected, we know who would be the mother and father to collect IQ score. In this way, both mother and father are related to the gifted children being selected. However, the IQ scores for each individual month and father are not related. If so, it would likely indicate a higher IQ in Mother would associated with a higher IQ in father (positive related) or lower IQ in father (negative related). A mother with average IQ score would likely associate with an average IQ father. \n",
    "\n",
    "(b) Conduct a hypothesis test to evaluate if the scores are equal on average. Make sure to clearly state your hypotheses, check the relevant conditions, and state your conclusion in the context of the data.  \n",
    "**Ans:**   \n",
    "set significance level $\\alpha = 0.05$  \n",
    "$H_0: \\mu_{diff} = 0$   \n",
    "$H_A: \\mu_{diff} \\ne 0$   \n",
    "check condition:  \n",
    "simple random sample implies that gifted child is randomly sample, which implies their mother and father's IQ score likely to be independent observations.  \n",
    "We are doing paired dependent means, so the difference distribution play a key role here. The distribution shows moderately right skewed, but n is large enough, so the sampling distribution of the paired difference likely support by CLT. Therefore, we can proceed with the paired means  framework  \n",
    "Calculate T-score and p-value:  \n",
    "$n = 36, df = 35, \\bar{x}_{diff}= 3.4, s_{diff} = 7.5, \\mu_{null} = 0$  \n",
    "$T = \\frac{\\bar{x}_{diff} - \\mu_{null}}{s_{diff}/\\sqrt{n}} = \\frac{3.4-0}{7.5/\\sqrt{36}}\\approx 2.72$   \n",
    "$p-value = 0.01 \\le 0.05$   \n",
    "Hence, reject null hypothesis. The evidence is strong enought to support that the mother's IQ and father's IQ are different for gifted children.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b047b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7199999999999998, 0.010095123310830179]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 36\n",
    "df = n - 1\n",
    "x_diff = 3.4\n",
    "s_diff = 7.5\n",
    "alpha = 0.5\n",
    "se_diff = s_diff / np.sqrt(n)\n",
    "t_score = (x_diff - 0)/se_diff\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93701692",
   "metadata": {},
   "source": [
    "**5.27 Friday the 13th, Part I.** In the early 1990’s, researchers in the UK collected data on traffic flow, number of shoppers, and traffic accident related emergency room admissions on Friday the 13th and the previous Friday, Friday the 6th. The histograms below show the distribution of number of cars passing by a specific intersection on Friday the 6th and Friday the 13th for many such date pairs. Also given are some sample statistics, where the difference is the number of cars on the 6th minus the number of cars on the 13th.  \n",
    "(a) Are there any underlying structures in these data that should be considered in an analysis? Explain.  \n",
    "**Ans:**   \n",
    "1) if both data follows normal distribution espically given a small sample size. \n",
    "2) for a sample size of 10, we would more relay on the normality of population in order to apply CLT. if the population distribuion is somehow moderately skewed, likely, the sampling distribution would inherit the skewness, which may make our test result less realiable.   \n",
    "\n",
    "(b) What are the hypotheses for evaluating whether the number of people out on Friday the 6th is different than the number out on Friday the 13th?  \n",
    "**Ans:**   \n",
    "$H_0: \\mu_6 - \\mu_{13} = 0$  \n",
    "$H_A: \\mu_6 - \\mu_{13} \\ne 0$\n",
    "\n",
    "(c) Check conditions to carry out the hypothesis test from part (b).  \n",
    "**Ans:**   \n",
    "Based on the research topic, one option would be use two independent population mean ttest. we should consider if data are independent within group and between group. Between group independency remains questions to me as it is a work-day, people may follow certain routine on Friday, so people who pass the specific intersection on 6th may also pass the specific intersection on 13th which make two group kinda dependent. However, we can not use dependent t-test as the evidence is not strong enough to indicate a pair-wise relationship.  \n",
    "because the sample only look at a specific intersection, which should be less than 10% of population who travel in either on 6th or 13th. \n",
    "\n",
    "given small sample size, normality of population become more important. The QQ plot for both 6th and 13th shows slight deviate from normalality which is probably acceptable for a sample size of 10 to continue to use t-test. However, we should be explicit about the limitation here and be cautious about the interpration of the test results. If possible, collecting more data points.\n",
    "\n",
    "(d) Calculate the test statistic and the p-value.  \n",
    "**Ans:**   \n",
    "$point\\;estimate = \\bar{x}_1-\\bar{x}_2 \\approx 1835$  \n",
    "\n",
    "$SE_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} = \\sqrt{\\frac{7259^2}{10} + \\frac{7664^2}{10}} \\approx3338.11$  \n",
    "\n",
    "$T = \\frac{\\bar{x}_1-\\bar{x}_2 - \\mu_{null}}{SE_{\\bar{x}_1-\\bar{x}_2}} = \\frac{128385-126550-0}{3338.11}\\approx0.5497$   \n",
    "\n",
    "$p-value \\approx 0.5959$ \n",
    "\n",
    "(e) What is the conclusion of the hypothesis test?   \n",
    "**Ans:**   \n",
    "The evidence is not convincing enough that the the number of people out on Friday the 6th is different than the number out on Friday the 13th.  \n",
    "\n",
    "(f) Interpret the p-value in this context.  \n",
    "**Ans:**   \n",
    "if the truth is no difference in terms of number of people out on Friday the 6th and the 13th, there is around 60% of chance to see at least 1,835 difference in number of people out between the two Fridays. \n",
    "\n",
    "(g) What type of error might have been made in the conclusion of your test? Explain.  \n",
    "**Ans:**   \n",
    "If may make a Type II error. Considerring the relative small sample size, the test may not got enough power to detect a true effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9aa4492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1835, 3338.112895035157]\n",
      "[0.549711785580779, 0.5958924611909167]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 10\n",
    "df = n - 1\n",
    "x_1 = 128385\n",
    "x_2 = 126550\n",
    "s_1 = 7259\n",
    "s_2 = 7664\n",
    "alpha = 0.5\n",
    "se = np.sqrt(np.power(s_1,2)/n + np.power(s_2,2)/n)\n",
    "t_score = (x_1-x_2 - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([x_1-x_2, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3091421",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c18745",
   "metadata": {},
   "source": [
    "**5.31 Chicken diet and weight, Part I.** Chicken farming is a multi-billion dollar industry, and any methods that increase the growth rate of young chicks can reduce consumer costs while increasing company profits, possibly by millions of dollars. An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. Newly hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. Below are some summary statistics from this data set along with box plots showing the distribution of weights by feed type.  \n",
    "    (a) Describe the distributions of weights of chickens that were fed linseed and horsebean.  \n",
    "    **Ans:**   \n",
    "    Linseed: slight right skewed with no obvious outliers  \n",
    "    horsebear: slight left skewed with no obvious outliers.   \n",
    "    \n",
    "(b) Do these data provide strong evidence that the average weights of chickens that were fed linseed and horsebean are different? Use a 5% significance level.   \n",
    "    **Ans:**   I would want to run ANOVA and use the MSE to calculate standard error when doing pair-wised post-hoc ttest if ANOVA is significance. \n",
    "Conditions:\n",
    "independence: random assignment implies independence likely hold both between and within group.  \n",
    "normality for each group: some sub-groups show slight skewness and one group show outlier, but overall, the data hold normality assumption.  \n",
    "homoscedastic: sample size are not that different between group, variance are not exactly the same but not that far away.  \n",
    "Therefore, proceed with ANOVA.  \n",
    "$MSE = \\frac{SSE}{n-k} = \\frac{(n_1-1)s_1^2 + ... + (n_6-1)s_6^2}{n-k}$  \n",
    "\n",
    "$MSB = \\frac{n_1(\\bar{x}_1 - \\bar{x})^2 + ... + n_6(\\bar{x}_6 - \\bar{x})^2}{k-1}$  \n",
    "\n",
    "$\\bar{x} = \\frac{\\sum_{i}x_i*n_i}{\\sum_{i}n_i}$  \n",
    "\n",
    "$F = \\frac{MSB}{MSE}$\n",
    "\n",
    "    \n",
    "$\\alpha = 0.05, n_1 = 12, n_2= 10, point\\;estimate =\\bar{x}_1-\\bar{x}_2 = 218.75-160.2\\approx 58.55$. \n",
    "\n",
    "$SE_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{MSE}{n_1} + \\frac{MSE}{n_2}}  \\approx23.4859$  \n",
    "\n",
    "$T = \\frac{\\bar{x}_1-\\bar{x}_2 - \\mu_{null}}{SE_{\\bar{x}_1-\\bar{x}_2}} = \\frac{58.55-0}{23.4859}\\approx2.4930$   \n",
    "\n",
    "$p-value \\approx 0.0130 < 0.05$  \n",
    "Hence, reject null hypothesis in favor of alternative hypothesis that the average weights of chickens that were fed linseed and horsebean are different.  \n",
    "\n",
    "(c) What type of error might we have committed? Explain.   \n",
    "    **Ans:**   \n",
    "    Type I error. We may fasely reject a true null hypothesis given our sample data.   \n",
    "    \n",
    "   (d) Would your conclusion change if we used α = 0.01?    \n",
    "    **Ans:**   \n",
    "    we fail to reject null hypothesis and claims that no obvious evidence indicates difference in average weight between chicken fed with linseed and horsebean. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1eed81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.364276649234473, 5.93984750274501e-10]\n"
     ]
    }
   ],
   "source": [
    "#ANOVA first\n",
    "import numpy as np\n",
    "import scipy \n",
    "x1 = 323.58;x2=160.2;x3 = 218.75;x4=276.91;x5=246.43;x6=328.92\n",
    "s1=64.43;s2=38.63;s3=52.24;s4=64.9;s5=54.13;s6=48.84\n",
    "n1=12;n2=10;n3=12;n4=11;n5=14;n6=12\n",
    "n = n1+n2+n3+n4+n5+n6\n",
    "k = 6\n",
    "x_bar = (x1*n1+x2*n2+x3*n3+x4*n4+x5*n5+x6*n6)/(n)\n",
    "mse = ((n1-1)*np.power(s1,2)+(n2-1)*np.power(s2,2)+(n3-1)*np.power(s3,2)+(n4-1)*np.power(s4,2)+(n5-1)*np.power(s5,2)+(n6-1)*np.power(s6,2))/(n - k)\n",
    "msb = (n1*np.power(x1-x_bar,2) + n2*np.power(x2-x_bar,2)+ n3*np.power(x3-x_bar,2)+ n4*np.power(x4-x_bar,2)+ n5*np.power(x5-x_bar,2)+ n6*np.power(x6-x_bar,2))/(k-1)\n",
    "f_score = msb/mse\n",
    "p_value = 1-scipy.stats.f.cdf(f_score,k-1,n-k)\n",
    "print([f_score,p_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1f3256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA first with vectorization\n",
    "x = [323.58,160.2, 218.75,276.91,246.43,328.92]\n",
    "s = [64.43,38.63,52.24,64.9,54.13,48.84]\n",
    "n = [12,10,12,11,14,12]\n",
    "x_bar = np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbe1d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.3866781152442, 5.79486791885131e-10]\n"
     ]
    }
   ],
   "source": [
    "msb = sum([n[i] * np.power(x[i] - x_bar,2) for i in range(6)])/(k-1)\n",
    "mse = sum([(n[i] - 1) * np.power(s[i],2) for i in range(6)])/(sum(n)-k)\n",
    "f_score = msb/mse\n",
    "p_value = 1-scipy.stats.f.cdf(f_score,k-1,sum(n)-k)\n",
    "print([f_score,p_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d557fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.55000000000001, 23.48589762291078]\n",
      "[2.492985405117485, 0.03182702433782736]\n"
     ]
    }
   ],
   "source": [
    "#post-hoc ttest after significant ANOVA\n",
    "import numpy as np\n",
    "import scipy \n",
    "n1 = 12\n",
    "n2= 10\n",
    "df = min(n1,n2)\n",
    "poe = 218.75-160.2 #point of estimate\n",
    "se = np.sqrt(mse/n1 + mse/n2)\n",
    "t_score = (poe - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([poe, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a947b4",
   "metadata": {},
   "source": [
    "**5.35 Gaming and distracted eating, Part I.** A group of researchers are interested in the possible effects of distracting stimuli during eating, such as an increase or decrease in the amount of food consumption. To test this hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal groups. The treatment group ate lunch while playing solitaire, and the control group ate lunch without any added distractions. Patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of 45.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 26.4 grams. Do these data provide convincing evidence that the average food intake (measured in amount of biscuits consumed) is different for the patients in the treatment group? Assume that conditions for inference are satisfied.  \n",
    "**Ans:**   \n",
    "We can use two sample mean ttest for this research topic, first, we need to check the conditions.  \n",
    "random sample and random assignment into two equal groups implies independence likely holds.  \n",
    "22 patients each group shows a moderate size sample, I would research the general pattern of food intake to validate the normality assumption. Here, i would go ahead to think CLT can apply in this topic given decent sample size and no obvious sign of outliers and skewness of the population distribution.  \n",
    "t-score = 2.2438\n",
    "p-value = 0.0352 < 0.05  \n",
    "hence, reject null hypothesis, the evidence supports that the average food intake is different for the patients in the treatment group.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dff0f5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.0, 11.14158875564881]\n",
      "[2.243845159634432, 0.035234622517193115]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n1 = 22\n",
    "n2= 22\n",
    "s_1 = 45.1\n",
    "s_2 = 26.4\n",
    "df = min(n1,n2)\n",
    "poe = 52.1-27.1 #point of estimate\n",
    "se = np.sqrt(np.power(s_1,2)/n1 + np.power(s_2,2)/n2)\n",
    "t_score = (poe - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([poe, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c157db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.37"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8793d3e",
   "metadata": {},
   "source": [
    "**5.37 Prison isolation experiment, Part I.** Subjects from Central Prison in Raleigh, NC, volunteered for an experiment involving an “isolation” experience. The goal of the experiment was to find a treatment that reduces subjects’ psychopathic deviant T scores. This score measures a person’s need for control or their rebellion against control, and it is part of a commonly used mental health test called the Minnesota Multiphasic Personality Inventory (MMPI) test. The experiment had three treatment groups:\n",
    "(1) Four hours of sensory restriction plus a 15minute “therapeutic” tape advising that professional help is available.\n",
    "(2) Four hours of sensory restriction plus a 15 minute “emotionally neutral” tape on training hunting dogs.\n",
    "(3) Four hours of sensory restriction but no taped message.\n",
    "\n",
    "Forty-two subjects were randomly assigned to these treatment groups, and an MMPI test was administered before and after the treatment. Distributions of the differences between pre and post treatment scores (pre - post) are shown below, along with some sample statistics. Use this information to independently test the effectiveness of each treatment. Make sure to clearly state your hypotheses, check conditions, and interpret results in the context of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1237766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Power: 5.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b55e08",
   "metadata": {},
   "source": [
    "Comparing three or more means (ANOVA): 5.41, 5.43, 5.45, 5.47, 5.49, 5.51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1983b",
   "metadata": {},
   "source": [
    "Simulation based inference for means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae42b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (realestate_env)",
   "language": "python",
   "name": "realestate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da8dc33",
   "metadata": {},
   "source": [
    "t-inference: 5.1, 5.3, 5.5, 5.13, 5.17, 5.19, 5.21, 5.23, 5.27, 5.31, 5.35, 5.37\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f531ba3",
   "metadata": {},
   "source": [
    "**5.1 Identify the critical t.** An independent random sample is selected from an <u>approximately normal population</u> with unknown standard deviation. Find the degrees of freedom and the critical t-value ($t^*$) for the given sample size and confidence level.  \n",
    " (a) n=6,CL=90%   \n",
    " (b) n=21,CL=98%   \n",
    " (c) n=29,CL=95%   \n",
    " (d) n=12,CL=99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1637c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.015048372669157\n",
      "2.527977002740546\n",
      "2.048407141795244\n",
      "3.1058065132211006\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.stats.t.ppf(0.95, 5))\n",
    "print(scipy.stats.t.ppf(0.99, 20))\n",
    "print(scipy.stats.t.ppf(0.975, 28))\n",
    "print(scipy.stats.t.ppf(0.995, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917cd88f",
   "metadata": {},
   "source": [
    "**5.3 Find the p-value, Part I.** An independent random sample is selected from an <u>approximately normal population</u> with an unknown standard deviation. Find the p-value for the given set of hypotheses and T test statistic. Also determine if the null hypothesis would be rejected at α = 0.05.  \n",
    "(a) HA :μ>μ0,n=11,T =1.91, reject null hypothesis.  \n",
    "(b) HA :μ<μ0,n=17,T =−3.45, reject null hypothesis.    \n",
    "(c) HA :μ̸=μ0,n=7,T =0.83, fail to reject null hypothesis.    \n",
    "(d) HA :μ>μ0,n=28,T =2.13, reject null hypothesis.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab746d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04260243832257815\n",
      "0.0016467857018984567\n",
      "0.4383084015659864\n",
      "0.021217692960949996\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(1 - scipy.stats.t.cdf(1.91,10))\n",
    "print(scipy.stats.t.cdf(-3.45,16))\n",
    "print(2 * (1 - scipy.stats.t.cdf(0.83,6)))\n",
    "print(1 - scipy.stats.t.cdf(2.13,27))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bf13c",
   "metadata": {},
   "source": [
    "**5.4 Find the p-value, Part II.** An independent random sample is selected from <u>an approximately normal population</u> with an unknown standard deviation. Find the p-value for the given set of hypotheses and T test statistic. Also determine if the null hypothesis would be rejected at α = 0.01.  \n",
    "(a) HA :μ>0.5,n=26,T =2.485, reject null hypothesis.  \n",
    "(b) HA :μ<3,n=18,T =0.5, fail to reject null hypothesis.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3896d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010002400548908885\n",
      "0.6882573967171443\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(1 - scipy.stats.t.cdf(2.485,25))\n",
    "print(scipy.stats.t.cdf(0.5,17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30140c8",
   "metadata": {},
   "source": [
    "**5.5 Working backwards, Part I.** A 95% confidence interval for a population mean, μ, is given as (18.985, 21.015). This confidence interval is based on a simple random sample of 36 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f9e86",
   "metadata": {},
   "source": [
    "**Ans:** we know lower bound = 18.985, upper bound = 21.015, n= 36, $t^* = 2.03$  \n",
    "$lower bound = \\bar{x} - t^* * \\frac{s}{\\sqrt{n}}$,  \n",
    "$upper bound = \\bar{x} + t^* * \\frac{s}{\\sqrt{n}}$    \n",
    "Hence,  \n",
    "$\\bar{x} = \\frac{lower bound + upper bound}{2} = 20$    \n",
    "\n",
    "$s = ( \\bar{x} - lower bound ) * \\frac{\\sqrt{n}}{t^*} \\approx 3$.   \n",
    "plug the known into the formula,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b0d020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0301079282503425"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "t_critical = scipy.stats.t.ppf(0.975, 35)\n",
    "t_critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a52d08ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0, 2.9998405086022677]\n"
     ]
    }
   ],
   "source": [
    "x_bar = (18.985+21.015)/2\n",
    "s = (x_bar - 18.985) * 6/t_critical\n",
    "print([x_bar,s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6a319",
   "metadata": {},
   "source": [
    "**5.13 Car insurance savings.** A market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $100$. He wants to collect data such that he can get a margin of error of no more than $10 at a 95% confidence level. How large of a sample should he collect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc25ca",
   "metadata": {},
   "source": [
    "**Ans:** $Margin of Error = 1.96 * 100/\\sqrt{n} = 10$  \n",
    "hence,  \n",
    "n = 385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff1aa51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1600000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.power(1.96*100/10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95462c34",
   "metadata": {},
   "source": [
    "**5.17 Paired or not, Part I?** In each of the following scenarios, determine if the data are paired.  \n",
    "(a) Compare pre- (beginning of semester) and post-test (end of semester) scores of students. **paired**  \n",
    "(b) Assess gender-related salary gap by comparing salaries of randomly sampled men and women. **not paired**.   \n",
    "(c) Compare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin E for the same group of patients. **paired**.   \n",
    "(d) Assess effectiveness of a diet regimen by comparing the before and after weights of subjects. **paired**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04954a77",
   "metadata": {},
   "source": [
    "**5.19 Global warming, Part I.** Is there strong evidence of global warming? Let’s consider a small scale example, comparing how temperatures have changed in the US from 1968 to 2008. The daily high temperature reading on January 1 was collected in 1968 and 2008 for 51 randomly selected locations in the continental US. Then the difference between the two readings (temperature in 2008 - temperature in 1968) was calculated for each of the 51 different locations. The average of these 51 values was 1.1 degrees with a standard deviation of 4.9 degrees. We are interested in determining whether these data provide strong evidence of temperature warming in the continental US.  \n",
    "(a) Is there a relationship between the observations collected in 1968 and 2008? Or are the observations in the two groups independent? Explain.  \n",
    "**Ans:** independent.  temperatures in one year do not impact temperatures in another year.   <span style=\"color:red\">dependent. Since for each obsercation in one data set, there is exactly one specially-corresponding observation in the other data set for the same geographic location, the data are paired</span>   \n",
    "\n",
    "(b) Write hypotheses for this research in symbols and in words.  \n",
    "**Ans:**  \n",
    "$$H_0: \\mu_{2008} - \\mu_{1968} = 0$$.  no temperature warming in the continential US  \n",
    "$$H_A: \\mu_{2008} - \\mu_{1968} \\gt 0$$. temperature is warmer in 2008 compared to 1968 in the continential US  \n",
    "  <span style=\"color:red\">$$H_0: \\mu_{diff} = 0$$</span>   \n",
    "    <span style=\"color:red\">$$H_0: \\mu_{diff} \\ne 0$$</span>   \n",
    "(c) Check the conditions required to complete this test.  \n",
    "**Ans:**  The 51 locations were randomly selected, so likely the observation in our sample is independent in both time points. sample size for both group is 51, so the sample size is considered large enough for CLT to be applied assuming the population distribution is not extremely skewed. If I have the dataset, I would run a histogram plot of both data set to check the shape and outliers. I would also check third party reseach in terms of temperature distribution in US continental to make sure the skewness assumption do not deviate too much.  \n",
    "\n",
    "(d) Calculate the test statistic and find the p-value.  \n",
    "**Ans:**  \n",
    "$$n = 51, x_{diff}  = 1.1, s_diff = 4.9, df = 50$$\n",
    "\n",
    "$$T = \\frac{x_{diff} - \\mu_0}{s_{diff}/\\sqrt{n}} = \\frac{1.1-0}{4.9/\\sqrt{51}} \\approx 1.6032$$\n",
    "\n",
    "$$ p-value \\approx 0.05760 \\gt 0.05$$\n",
    "\n",
    "(e) What do you conclude? Interpret your conclusion in context.  \n",
    "**Ans:**  \n",
    "Failed to reject null hypothesis. The evidence does not strong enough to support the claim that there is global warming in the continental US.  \n",
    "\n",
    "(f) What type of error might we have made? Explain in context what the error means.  \n",
    "**Ans:**  \n",
    "We may make a Type II error which means our sample data did not capture the global warming phenomenon when there is truely global warming in the continental US.  \n",
    "\n",
    "(g) Based on the results of this hypothesis test, would you expect a confidence interval for the average difference between the temperature measurements from 1968 and 2008 to include 0? Explain your reasoning.  \n",
    "**Ans:**  \n",
    "Yes. a confidence interval would include the null value when the hypothesis test fails to reject null hypothesis.  In this case, the null value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12297aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6031778104892114\n",
      "0.05759730789200801\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "t_score = (1.1-0)/(4.9/np.sqrt(51))\n",
    "print(t_score)\n",
    "print(1 - scipy.stats.t.cdf(t_score, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06ba50",
   "metadata": {},
   "source": [
    "**5.21 Global warming, Part II.** We considered the differences between the temperature readings in January 1 of 1968 and 2008 at 51 locations in the continental US in Exercise 5.19. The mean and standard deviation of the reported differences are 1.1 degrees and 4.9 degrees.  \n",
    "(a) Calculate a 90% confidence interval for the average difference between the temperature measurements between 1968 and 2008.  \n",
    "**Ans:**   \n",
    "[-0.0499, 2.2499]  \n",
    "\n",
    "(b) Interpret this interval in context.  \n",
    "**Ans:**   \n",
    "we are 90% confidence that the average difference between 1968 and 2008 in the US continent is between [-0.0499, 2.2499] degrees. \n",
    "\n",
    "(c) Does the confidence interval provide convincing evidence that the temperature was higher in 2008 than in 1968 in the continental US? Explain.  \n",
    "**Ans:**   \n",
    "Because the confidence interval include 0, in the context of hypothesis test, we fail to reject the null hypothesis which claims there is no difference in tempature between 1968 and 2008 in contenental US.  Therefore, CI does NOT provide convincing evidence that the temperature was higher in 2008 than in 1968 in the continental US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2a45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049900849999052754, 2.2499008499990527]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 51\n",
    "df = n - 1\n",
    "x_diff = 1.1\n",
    "s_diff = 4.9\n",
    "alpha = 0.1\n",
    "t_critical = scipy.stats.t.ppf(1-alpha/2,df)\n",
    "se_diff = s_diff / np.sqrt(n)\n",
    "print([x_diff-t_critical * se_diff, x_diff + t_critical * se_diff])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae488b39",
   "metadata": {},
   "source": [
    "**5.23 Gifted children.** Researchers collected a simple random sample of 36 children who had been identified as gifted in a large city. The following histograms show the distributions of the IQ scores of mothers and fathers of these children. Also provided are some sample statistics.  \n",
    "(a) Are the IQs of mothers and the IQs of fathers in this data set related? Explain.  \n",
    "**Ans:**  \n",
    "children is randomly selected, when selected, we know who would be the mother and father to collect IQ score. In this way, both mother and father are related to the gifted children being selected.  ~~<span style=\"color:red\">However, the IQ scores for each individual month and father are not related. If so, it would likely indicate a higher IQ in Mother would associated with a higher IQ in father (positive related) or lower IQ in father (negative related). A mother with average IQ score would likely associate with an average IQ father. </span>~~\n",
    "\n",
    "(b) Conduct a hypothesis test to evaluate if the scores are equal on average. Make sure to clearly state your hypotheses, check the relevant conditions, and state your conclusion in the context of the data.  \n",
    "**Ans:**   \n",
    "set significance level $\\alpha = 0.05$  \n",
    "$H_0: \\mu_{diff} = 0$   \n",
    "$H_A: \\mu_{diff} \\ne 0$   \n",
    "check condition:  \n",
    "simple random sample implies that gifted child is randomly sample, which implies their mother and father's IQ score likely to be independent observations.  \n",
    "We are doing paired dependent means, so the difference distribution play a key role here. The distribution shows moderately right skewed, but n is large enough, so the sampling distribution of the paired difference likely support by CLT. Therefore, we can proceed with the paired means  framework  \n",
    "Calculate T-score and p-value:  \n",
    "$n = 36, df = 35, \\bar{x}_{diff}= 3.4, s_{diff} = 7.5, \\mu_{null} = 0$  \n",
    "$T = \\frac{\\bar{x}_{diff} - \\mu_{null}}{s_{diff}/\\sqrt{n}} = \\frac{3.4-0}{7.5/\\sqrt{36}}\\approx 2.72$   \n",
    "$p-value = 0.01 \\le 0.05$   \n",
    "Hence, reject null hypothesis. The evidence is strong enought to support that the mother's IQ and father's IQ are different for gifted children.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "144de84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7199999999999998, 0.010095123310830179]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 36\n",
    "df = n - 1\n",
    "x_diff = 3.4\n",
    "s_diff = 7.5\n",
    "alpha = 0.5\n",
    "se_diff = s_diff / np.sqrt(n)\n",
    "t_score = (x_diff - 0)/se_diff\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c9760",
   "metadata": {},
   "source": [
    "**5.27 Friday the 13th, Part I.** In the early 1990’s, researchers in the UK collected data on traffic flow, number of shoppers, and traffic accident related emergency room admissions on Friday the 13th and the previous Friday, Friday the 6th. The histograms below show the distribution of number of cars passing by a specific intersection on Friday the 6th and Friday the 13th for many such date pairs. Also given are some sample statistics, where the difference is the number of cars on the 6th minus the number of cars on the 13th.  \n",
    "(a) Are there any underlying structures in these data that should be considered in an analysis? Explain.  \n",
    "<span style=\"color:red\">These data are paired. For example, the Friday the 13th in say, September 1991, would probably be more similar to the Fri- day the 6th in September 1991 than to Fri- day the 6th in another month or year.It's very interesting to see when using paired t-test, T-scode = 4.94 as compared to using independent two sample t-test, it probably due to paired t-test remore varibality between different observation, therefore, leads to more power in detecting the effect</span>  \n",
    "**Ans:**   \n",
    "1) if both data follows normal distribution espically given a small sample size. \n",
    "2) for a sample size of 10, we would more relay on the normality of population in order to apply CLT. if the population distribuion is somehow moderately skewed, likely, the sampling distribution would inherit the skewness, which may make our test result less realiable.   \n",
    "\n",
    "(b) What are the hypotheses for evaluating whether the number of people out on Friday the 6th is different than the number out on Friday the 13th?  \n",
    "**Ans:**   \n",
    "$H_0: \\mu_6 - \\mu_{13} = 0$  \n",
    "$H_A: \\mu_6 - \\mu_{13} \\ne 0$\n",
    "\n",
    "(c) Check conditions to carry out the hypothesis test from part (b).  \n",
    "**Ans:**   \n",
    "Based on the research topic, one option would be use two independent population mean ttest. we should consider if data are independent within group and between group. Between group independency remains questions to me as it is a work-day, people may follow certain routine on Friday, so people who pass the specific intersection on 6th may also pass the specific intersection on 13th which make two group kinda dependent. However, we can not use dependent t-test as the evidence is not strong enough to indicate a pair-wise relationship.  \n",
    "because the sample only look at a specific intersection, which should be less than 10% of population who travel in either on 6th or 13th. \n",
    "\n",
    "given small sample size, normality of population become more important. The QQ plot for both 6th and 13th shows slight deviate from normalality which is probably acceptable for a sample size of 10 to continue to use t-test. However, we should be explicit about the limitation here and be cautious about the interpration of the test results. If possible, collecting more data points.\n",
    "\n",
    "(d) Calculate the test statistic and the p-value.  \n",
    "**Ans:**   \n",
    "$point\\;estimate = \\bar{x}_1-\\bar{x}_2 \\approx 1835$  \n",
    "\n",
    "$SE_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} = \\sqrt{\\frac{7259^2}{10} + \\frac{7664^2}{10}} \\approx3338.11$  \n",
    "\n",
    "$T = \\frac{\\bar{x}_1-\\bar{x}_2 - \\mu_{null}}{SE_{\\bar{x}_1-\\bar{x}_2}} = \\frac{128385-126550-0}{3338.11}\\approx0.5497$   \n",
    "\n",
    "$p-value \\approx 0.5959$ \n",
    "\n",
    "(e) What is the conclusion of the hypothesis test?   \n",
    "**Ans:**   \n",
    "The evidence is not convincing enough that the the number of people out on Friday the 6th is different than the number out on Friday the 13th.  \n",
    "\n",
    "(f) Interpret the p-value in this context.  \n",
    "**Ans:**   \n",
    "if the truth is no difference in terms of number of people out on Friday the 6th and the 13th, there is around 60% of chance to see at least 1,835 difference in number of people out between the two Fridays. \n",
    "\n",
    "(g) What type of error might have been made in the conclusion of your test? Explain.  \n",
    "**Ans:**   \n",
    "If may make a Type II error. Considerring the relative small sample size, the test may not got enough power to detect a true effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebf2c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1835, 3338.112895035157]\n",
      "[0.549711785580779, 0.5958924611909167]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n = 10\n",
    "df = n - 1\n",
    "x_1 = 128385\n",
    "x_2 = 126550\n",
    "s_1 = 7259\n",
    "s_2 = 7664\n",
    "alpha = 0.5\n",
    "se = np.sqrt(np.power(s_1,2)/n + np.power(s_2,2)/n)\n",
    "t_score = (x_1-x_2 - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([x_1-x_2, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fea44",
   "metadata": {},
   "source": [
    "**5.31 Chicken diet and weight, Part I.** Chicken farming is a multi-billion dollar industry, and any methods that increase the growth rate of young chicks can reduce consumer costs while increasing company profits, possibly by millions of dollars. An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. Newly hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. Below are some summary statistics from this data set along with box plots showing the distribution of weights by feed type.  \n",
    "    (a) Describe the distributions of weights of chickens that were fed linseed and horsebean.  \n",
    "    **Ans:**   \n",
    "    Linseed: slight right skewed with no obvious outliers  \n",
    "    horsebear: slight left skewed with no obvious outliers.   \n",
    "    \n",
    "(b) Do these data provide strong evidence that the average weights of chickens that were fed linseed and horsebean are different? Use a 5% significance level.   \n",
    "    **Ans:**   I would want to run ANOVA and use the MSE to calculate standard error when doing pair-wised post-hoc ttest if ANOVA is significance. \n",
    "Conditions:\n",
    "independence: random assignment implies independence likely hold both between and within group.  \n",
    "normality for each group: some sub-groups show slight skewness and one group show outlier, but overall, the data hold normality assumption.  \n",
    "homoscedastic: sample size are not that different between group, variance are not exactly the same but not that far away.  \n",
    "Therefore, proceed with ANOVA.  \n",
    "$MSE = \\frac{SSE}{n-k} = \\frac{(n_1-1)s_1^2 + ... + (n_6-1)s_6^2}{n-k}$  \n",
    "\n",
    "$MSB = \\frac{n_1(\\bar{x}_1 - \\bar{x})^2 + ... + n_6(\\bar{x}_6 - \\bar{x})^2}{k-1}$  \n",
    "\n",
    "$\\bar{x} = \\frac{\\sum_{i}x_i*n_i}{\\sum_{i}n_i}$  \n",
    "\n",
    "$F = \\frac{MSB}{MSE}$\n",
    "\n",
    "    \n",
    "$\\alpha = 0.05, n_1 = 12, n_2= 10, point\\;estimate =\\bar{x}_1-\\bar{x}_2 = 218.75-160.2\\approx 58.55$. \n",
    "\n",
    "$SE_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{\\frac{MSE}{n_1} + \\frac{MSE}{n_2}}  \\approx23.4859$  \n",
    "\n",
    "$T = \\frac{\\bar{x}_1-\\bar{x}_2 - \\mu_{null}}{SE_{\\bar{x}_1-\\bar{x}_2}} = \\frac{58.55-0}{23.4859}\\approx2.4930$   \n",
    "\n",
    "$p-value \\approx 0.0130 < 0.05$  \n",
    "Hence, reject null hypothesis in favor of alternative hypothesis that the average weights of chickens that were fed linseed and horsebean are different.  \n",
    "\n",
    "(c) What type of error might we have committed? Explain.   \n",
    "    **Ans:**   \n",
    "    Type I error. We may fasely reject a true null hypothesis given our sample data.   \n",
    "    \n",
    "   (d) Would your conclusion change if we used α = 0.01?    \n",
    "    **Ans:**   \n",
    "    we fail to reject null hypothesis and claims that no obvious evidence indicates difference in average weight between chicken fed with linseed and horsebean. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a289f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.364276649234473, 5.93984750274501e-10]\n"
     ]
    }
   ],
   "source": [
    "#ANOVA first\n",
    "import numpy as np\n",
    "import scipy \n",
    "x1 = 323.58;x2=160.2;x3 = 218.75;x4=276.91;x5=246.43;x6=328.92\n",
    "s1=64.43;s2=38.63;s3=52.24;s4=64.9;s5=54.13;s6=48.84\n",
    "n1=12;n2=10;n3=12;n4=11;n5=14;n6=12\n",
    "n = n1+n2+n3+n4+n5+n6\n",
    "k = 6\n",
    "x_bar = (x1*n1+x2*n2+x3*n3+x4*n4+x5*n5+x6*n6)/(n)\n",
    "mse = ((n1-1)*np.power(s1,2)+(n2-1)*np.power(s2,2)+(n3-1)*np.power(s3,2)+(n4-1)*np.power(s4,2)+(n5-1)*np.power(s5,2)+(n6-1)*np.power(s6,2))/(n - k)\n",
    "msb = (n1*np.power(x1-x_bar,2) + n2*np.power(x2-x_bar,2)+ n3*np.power(x3-x_bar,2)+ n4*np.power(x4-x_bar,2)+ n5*np.power(x5-x_bar,2)+ n6*np.power(x6-x_bar,2))/(k-1)\n",
    "f_score = msb/mse\n",
    "p_value = 1-scipy.stats.f.cdf(f_score,k-1,n-k)\n",
    "print([f_score,p_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c409df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA first with vectorization\n",
    "x = [323.58,160.2, 218.75,276.91,246.43,328.92]\n",
    "s = [64.43,38.63,52.24,64.9,54.13,48.84]\n",
    "n = [12,10,12,11,14,12]\n",
    "x_bar = np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be3c0979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.3866781152442, 5.79486791885131e-10]\n"
     ]
    }
   ],
   "source": [
    "msb = sum([n[i] * np.power(x[i] - x_bar,2) for i in range(6)])/(k-1)\n",
    "mse = sum([(n[i] - 1) * np.power(s[i],2) for i in range(6)])/(sum(n)-k)\n",
    "f_score = msb/mse\n",
    "p_value = 1-scipy.stats.f.cdf(f_score,k-1,sum(n)-k)\n",
    "print([f_score,p_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "83016672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.55000000000001, 4.1932684149717865]\n",
      "[13.962855273216263, 2.098447304810236e-07]\n"
     ]
    }
   ],
   "source": [
    "#post-hoc ttest after significant ANOVA\n",
    "import numpy as np\n",
    "import scipy \n",
    "n1 = 12\n",
    "n2= 10\n",
    "df = min(n1-1,n2-1)\n",
    "poe = 218.75-160.2 #point of estimate\n",
    "se = np.sqrt(mse/n1 + mse/n2)\n",
    "t_score = (poe - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([poe, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5199c",
   "metadata": {},
   "source": [
    "**5.35 Gaming and distracted eating, Part I.** A group of researchers are interested in the possible effects of distracting stimuli during eating, such as an increase or decrease in the amount of food consumption. To test this hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal groups. The treatment group ate lunch while playing solitaire, and the control group ate lunch without any added distractions. Patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of 45.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 26.4 grams. Do these data provide convincing evidence that the average food intake (measured in amount of biscuits consumed) is different for the patients in the treatment group? Assume that conditions for inference are satisfied.  \n",
    "**Ans:**   \n",
    "We can use two sample mean ttest for this research topic, first, we need to check the conditions.  \n",
    "random sample and random assignment into two equal groups implies independence likely holds.  \n",
    "22 patients each group shows a moderate size sample, I would research the general pattern of food intake to validate the normality assumption. Here, i would go ahead to think CLT can apply in this topic given decent sample size and no obvious sign of outliers and skewness of the population distribution.  \n",
    "t-score = 2.2438\n",
    "p-value = 0.0352 < 0.05  \n",
    "hence, reject null hypothesis, the evidence supports that the average food intake is different for the patients in the treatment group.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "041a3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.0, 11.14158875564881]\n",
      "[2.243845159634432, 0.03575082267141538]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "n1 = 22\n",
    "n2= 22\n",
    "s_1 = 45.1\n",
    "s_2 = 26.4\n",
    "df = min(n1-1,n2-1)\n",
    "poe = 52.1-27.1 #point of estimate\n",
    "se = np.sqrt(np.power(s_1,2)/n1 + np.power(s_2,2)/n2)\n",
    "t_score = (poe - 0)/se\n",
    "p_value = (1 - scipy.stats.t.cdf(t_score, df)) * 2\n",
    "print([poe, se])\n",
    "print([t_score, p_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f2914",
   "metadata": {},
   "source": [
    "**5.37 Prison isolation experiment, Part I.** Subjects from Central Prison in Raleigh, NC, volunteered for an experiment involving an “isolation” experience. The goal of the experiment was to find a treatment that reduces subjects’ psychopathic deviant T scores. This score measures a person’s need for control or their rebellion against control, and it is part of a commonly used mental health test called the Minnesota Multiphasic Personality Inventory (MMPI) test. The experiment had three treatment groups:\n",
    "(1) Four hours of sensory restriction plus a 15minute “therapeutic” tape advising that professional help is available.\n",
    "(2) Four hours of sensory restriction plus a 15 minute “emotionally neutral” tape on training hunting dogs.\n",
    "(3) Four hours of sensory restriction but no taped message.\n",
    "\n",
    "Forty-two subjects were randomly assigned to these treatment groups, and an MMPI test was administered before and after the treatment. Distributions of the differences between pre and post treatment scores (pre - post) are shown below, along with some sample statistics. Use this information to independently test the effectiveness of each treatment. Make sure to clearly state your hypotheses, check conditions, and interpret results in the context of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d0b29",
   "metadata": {},
   "source": [
    "**Ans** The normality of population do not hold based on the sample data distribution, however, given sample size is 14, ttest would probably still robust for distribution that is moderate deviated from normal.  \n",
    "independence likely holds as the sample size is less than 10% population.  \n",
    "based on the paired ttest, treatment 3 has different effects on person's need for control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2df68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [6.21,2.86,-3.21]\n",
    "s = [12.3,7.94,8.57]\n",
    "n = [14,14,14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "26e3d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment 1 vs treatment 2\n",
      "[3.35, 3.9127410925261663, 0.4063158045803017, 'test result: fail to reject null hypothesis.']\n",
      "treatment 1 vs treatment 3\n",
      "[9.42, 4.006556234117133, 0.03388930158858394, 'test result: reject null hypothesis.']\n",
      "treatment 2 vs treatment 3\n",
      "[6.07, 3.1223674625880555, 0.07226715413303086, 'test result: fail to reject null hypothesis.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x)):\n",
    "    for j in range(len(x)):\n",
    "        if i < j:\n",
    "            print(\"treatment {} vs treatment {}\".format(i+1,j+1))\n",
    "            poe = x[i] - x[j]\n",
    "            se = np.sqrt(np.power(s[i],2)/n[i] + np.power(s[j],2)/n[j])\n",
    "            t_score = (poe-0)/se\n",
    "            p_value = (1 - scipy.stats.t.cdf(abs(t_score), min(n[i],n[j]))) * 2\n",
    "            result = [\"test result: reject null hypothesis.\" if p_value < 0.05 or p_value == 0.05 else \"test result: fail to reject null hypothesis.\"]\n",
    "            print([poe,se,p_value, result[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b936be6",
   "metadata": {},
   "source": [
    "Power: 5.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b62a184",
   "metadata": {},
   "source": [
    "**5.39 Increasing corn yield.** A large farm wants to try out a new type of fertilizer to evaluate whether it will improve the farm’s corn production. The land is broken into plots that produce an average of 1,215 pounds of corn with a standard deviation of 94 pounds per plot. The owner is interested in detecting any average difference of at least 40 pounds per plot. How many plots of land would be needed for the experiment if the desired power level is 90%? Assume each plot of land gets treated with either the current fertilizer or the new fertilizer.\n",
    "\n",
    "**Ans:** 116 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a508a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.959963984540054, 1.2815515655446004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116.05448771361166"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "alpha = 0.05\n",
    "power = 0.9\n",
    "effect_size = 40\n",
    "mu = 1215\n",
    "s = 94\n",
    "z_critical_null = scipy.stats.norm.ppf(1-alpha/2)\n",
    "z_critical_null\n",
    "z_critical_alter = abs(scipy.stats.norm.ppf(1-power)) \n",
    "print([z_critical_null, z_critical_alter])\n",
    "#SE = effect / (z_critical_null + z_critical_alter)\n",
    "\n",
    "n =2* np.power( s * (z_critical_null + z_critical_alter)/effect_size, 2)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb3ae9",
   "metadata": {},
   "source": [
    "Comparing three or more means (ANOVA): 5.41, 5.43, 5.45, 5.47, 5.49, 5.51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263512ca",
   "metadata": {},
   "source": [
    "**5.41 Fill in the blank.** When doing an ANOVA, you observe large differences in means between groups. Within the ANOVA framework, this would most likely be interpreted as evidence strongly favoring the <u> alternative </u> hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc674a35",
   "metadata": {},
   "source": [
    "**5.43 Chicken diet and weight, Part III.** In Exercises 5.31 and 5.33 we compared the effects of two types of feed at a time. A better analysis would first consider all feed types at once: casein, horsebean, linseed, meat meal, soybean, and sunflower. The ANOVA output below can be used to test for differences between the average weights of chicks on different diets. Conduct a hypothesis test to determine if these data provide convincing evidence that the average weight of chicks varies across some (or all) groups. Make sure to check relevant conditions. Figures and summary statistics are shown below.\n",
    "**Ans:** Already done ANOVA analysis in exercise 5.31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631991c",
   "metadata": {},
   "source": [
    "**5.45 Coffee, depression, and physical activity.** Caffeine is the world’s most widely used stimulant, with approximately 80% consumed in the form of coffee. Participants in a study inves- tigating the relationship between coffee consumption and exercise were asked to report the number of hours they spent per week on moderate (e.g., brisk walking) and vigorous (e.g., strenuous sports and jogging) exercise. Based on these data the researchers estimated the total hours of metabolic equivalent tasks (MET) per week, a value always greater than 0. The table below gives summary statistics of MET for women in this study based on the amount of coffee consumed.  \n",
    "(a) Write the hypotheses for evaluating if the average physical activity level varies among the different levels of coffee consumption.  \n",
    "**Ans:**   \n",
    "$$H_0: \\mu_1 = \\mu_2 = ... = \\mu_5$$\n",
    "$$H_A: at\\;least\\;one\\;group\\;shows\\;different\\;physical\\;activity\\;level $$\n",
    "\n",
    "(b) Check conditions and describe any assumptions you must make to proceed with the test.  \n",
    "**Ans:**   \n",
    "<u>independence:</u> The way to group women itself ensure independent between group. Withingroup, it has not mentioned if the selection of participants is random or not, but the sample size should be less than 10% of the total population who either consume or not consume caffeinated coffee, therefore, the independence condition within group is likely hold.  \n",
    "<u>normality of population:</u> given the sample size is pretty large for each group, CLT is likely applied for each group sample mean.  \n",
    "<u>constance variance:</u> the reported sample standard deviation are pretty close to each other and given the large sample size, sample variance is likely to be a good estimator to the population variance, which implies the population variance between groups are likely to be the same.  \n",
    "with all 3 conditions checked and satisfied, we can go ahead to proceed the ANOAVA analysis.  \n",
    "set significance level to be 0.05\n",
    "(c) Below is part of the output associated with this test. Fill in the empty cells.  \n",
    "**Ans:**   \n",
    "DF_between = 4, DF_within = ~~50,739-4~~<span style=\"color:red\"> n-k = 50,739-5 = 50,734 </span>.   \n",
    "SSB = 25,575,327-25,564,819 = 10,508  \n",
    "MSB = 10,508/4 = 2,627  \n",
    "MSE = ~~25,564,819/50,735 = 503~~ <span style=\"color:red\"> = 25,564,819/50,734 = 503</span>.   \n",
    "F = MSB/MSE = 5.22  \n",
    "\n",
    "(d) What is the conclusion of the test?  \n",
    "**Ans:**   \n",
    "given p-value is less than the significant level (0.05), we think the data provide strong evidence to support alternative hypothesis that activity level differ in at least one level of the coffee consumption level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11f75c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.222664015904573"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25575327-25564819\n",
    "10508/4\n",
    "25564819/50735\n",
    "2627/503"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b2b87",
   "metadata": {},
   "source": [
    "**5.47 GPA and major.** Undergraduate students taking an introductory statistics course at Duke University conducted a survey about GPA and major. The side-by-side box plots show the distribution of GPA among three groups of majors. Also provided is the ANOVA output.  \n",
    "(a) Write the hypotheses for testing for a difference between average GPA across majors.   \n",
    "**Ans:**  \n",
    "$$H_0: \\mu_1 = \\mu_2 = \\mu_3$$\n",
    "$$H_A: at\\;least\\;one\\;major\\;has\\;different\\;GPA\\;for\\;the\\;introductory\\;statistics\\;course $$\n",
    "\n",
    "(b) What is the conclusion of the hypothesis test?  \n",
    "**Ans:**  \n",
    "given the large p-value, the evidence fail to support the claims that GPA for the statistics course diffe by majors.  \n",
    "\n",
    "(c) How many students answered these questions on the survey, i.e. what is the sample size?   \n",
    "**Ans:**   \n",
    "n - k = 195, k = 3, hence n = 198"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512426f",
   "metadata": {},
   "source": [
    "**5.49 True / False: ANOVA, Part I.** Determine if the following statements are true or false in ANOVA, and explain your reasoning for statements you identify as false.  \n",
    "(a) As the number of groups increases, the modified significance level for pairwise tests increases as well.  **Ans:** False, decrease, $\\alpha* = \\frac{\\alpha}{k(k-1)/2}$  \n",
    "\n",
    "(b) As the total sample size increases, the degrees of freedom for the residuals increases as well.   **Ans:** True. DF_within = n - k, when holding k as constant, large n leads to larger defree of freedom.  \n",
    "\n",
    "(c) The constant variance condition can be somewhat relaxed when the sample sizes are relatively consistent across groups.  \n",
    "<span style=\"color:red\"> True</span>.   \n",
    " **Ans:**  False.  even the sample size is similar, if sample size is small and constance variance does not hold, the ANOVA could lead to unrealiable results.  \n",
    " \n",
    "(d) The independence assumption can be relaxed when the total sample size is large.  **Ans:**  False. independence need to be hold, otherwise, the variance calculated could be off, leading to unrealiable test results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5b89f",
   "metadata": {},
   "source": [
    "**5.51 Prison isolation experiment, Part II.**  Exercise 5.37 introduced an experiment that was conducted with the goal of identifying a treatment that reduces subjects’ psychopathic deviant T scores, where this score measures a person’s need for control or his rebellion against control. In Exercise 5.37 you evaluated the success of each treatment individually. An alternative analysis involves comparing the success of treatments. The relevant ANOVA output is given below.\n",
    "(a) What are the hypotheses?  \n",
    "**Ans:**  \n",
    "$$H_0: \\mu_1 = \\mu_2 = \\mu_3$$\n",
    "$$H_A: at\\;least\\;one\\;treatment\\;has\\;different\\;effect\\;on\\;a\\;person’s\\;need\\;for\\; control\\; or\\; his\\; rebellion\\; against\\; control$$  \n",
    "(b) What is the conclusion of the test? Use a 5% significance level.  \n",
    "**Ans:**  \n",
    "at least one treatment has different effect on person's need for control.  \n",
    "\n",
    "(c) If in part (b) you determined that the test is significant, conduct pairwise tests to determine which groups are different from each other. If you did not reject the null hypothesis in part (b), recheck your answer.\n",
    "**Ans:**  \n",
    "<span style=\"color:red\"> the significance level should be adjusted to be 0.05/(2*3)/2 = 0.0167 </span>.   \n",
    "treatment 1 vs. treatment 2  \n",
    "p-value is 0.38 > ~~0.05.~~   <span style=\"color:red\"> 0.0167 </span>.   \n",
    "treatment 1 vs. treatment 3  \n",
    "p-value is 0.02 < ~~0.05.~~   <span style=\"color:red\"> 0.0167 </span>.   \n",
    "treatment 2 vs. treatment 3  \n",
    "p-value is 0.12 > ~~0.05.~~   <span style=\"color:red\"> 0.0167 </span>.   \n",
    "~~group 3 is different from group 1 and group 2~~\n",
    "<span style=\"color:red\"> conclusion: we have identified Treatment 1 and Treatment 3 as having different effects. Checking the other two comparisons, the differences are not statistically significant.</span>.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "724007ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 95.91\n",
    "msb = 319.74\n",
    "df_b = 2\n",
    "df_w = 39\n",
    "n=df_b + df_w\n",
    "n1 = n2 = n3 = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7f86022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26936439307871707"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treatment 1 vs. treatment 2\n",
    "poe = 6.21-2.86\n",
    "se = np.sqrt(mse/n1 + mse/n2)\n",
    "t_score = poe/se\n",
    "p_value = (1 - scipy.stats.t.cdf(abs(t_score), min(n1-1,n2-1))) * 2\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9a27c085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014646940557297095"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treatment 1 vs. treatment 3\n",
    "poe = 6.21+3.21\n",
    "se = np.sqrt(mse/n1 + mse/n3)\n",
    "t_score = poe/se\n",
    "p_value = (1 - scipy.stats.t.cdf(abs(t_score), min(n1-1,n3-1))) * 2\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8009e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09298877360865943"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treatment 2 vs. treatment 3\n",
    "poe = 2.86+3.21\n",
    "se = np.sqrt(mse/n2 + mse/n3)\n",
    "t_score = poe/se\n",
    "p_value = (1 - scipy.stats.t.cdf(abs(t_score), min(n2-1,n3-1))) * 2\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225902b",
   "metadata": {},
   "source": [
    "1. What is the t⋆ for a 95% confidence interval for a mean, where the sample size is 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4e4a8585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1788128296634177"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.stats.t.ppf(0.975,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36b1d6",
   "metadata": {},
   "source": [
    "2. What is the p-value for a hypothesis test where the alternative hypothesis is two-sided, the sample size is 20, and the test statistic, T, is calculated to be 1.75? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3e5a907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04812570319118337"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "1 - scipy.stats.t.cdf(1.75,19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6103c",
   "metadata": {},
   "source": [
    "3. 20 cardiac patients’ blood pressure is measured before taking a medication, and after. For a given patient, are the before and after blood pressure measurements dependent (paired) or independent? **paired**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b75080",
   "metadata": {},
   "source": [
    "4. A random sample of 100 students were obtained and then randomly assigned into two equal sized groups. One group went on a roller coaster while the other in a simulator at an amusement park. Afterwards their blood pressure measurements were taken. Are the measurements dependent (paired) or independent? **independent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a799d",
   "metadata": {},
   "source": [
    "5. Describe how the two sample means test is different from the paired means test, both conceptually and in terms of the calculation of the standard error.  \n",
    "**Two-Sample Means Test: Compares two separate groups and uses the variance within each group to calculate the standard error.  \n",
    "Paired Means Test: Compares two related sets of observations and uses the variance of the differences between pairs to calculate the standard error.  \n",
    "The key distinction lies in the independence and pairing of the observations. The paired means test inherently controls for inter-subject variability because it is based on the differences within paired or repeated measures on the same subjects, leading to a potentially more sensitive test if the pairing is appropriate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a67685",
   "metadata": {},
   "source": [
    "6. A 95% confidence interval for the difference between the number of calories consumed by mature and juvenile cats (μmat−μjuv) is (80 calories, 100 calories). Interpret this interval, and determine if it suggests a significant difference between the two means. **yes, it suggests significant difference between the two means as the 95% confidence interval does NOT include 0.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acd6e8",
   "metadata": {},
   "source": [
    "7. We would like to compare the average income of Americans who live in the Northeast, Midwest, South, and West. What are the appropriate hypotheses? **since the comparison is across 4 groups, ANOVA test is appropriate. If significant, post-hoc pairwised ttest with adjust significance level can be conducted to identify specific group that differ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ab2fd",
   "metadata": {},
   "source": [
    "8. Suppose the sample in the question above has 1000 observations, what are the degrees of freedom associated with the F-statistic? **F(3, 996)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3d789",
   "metadata": {},
   "source": [
    "9. Suppose the null hypothesis is rejected. Describe how we would discover which regions’ averages are different from each other. Make sure to discuss how many pairwise comparisons we would need to make, and what the corrected significance level would be. **4*3/2 = 6 pairs of t-test, $\\alpha^* = 0.05/6  \\approx 0.0833$ **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0c1ad",
   "metadata": {},
   "source": [
    "10. What visualizations are useful for checking each of the conditions required for performing ANOVA? **box plot is useful to check symemtric and outliers and constant variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016d5e0",
   "metadata": {},
   "source": [
    "11. How is a bootstrap distribution different from a sampling distribution? **bootstrap uses sample with replacement technique to empirically describe the sampling distribution of a sample statics of size n. Normally it would require use of computer program to run thousands of sample with replacement simulations and calculate the sample statics from each simulation, which would be used to construct the bootstrap distribution. The sampling distribution typically refers to approximate the distribution of a sample statistics of size n to  a well-defined distribution when underlining population meet certain condition. The sampling distribution typically involve using CLT and LLN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb5b38",
   "metadata": {},
   "source": [
    "12. If a bootstrap distribution is constructed using 200 simulations, how would we find the 95% bootstrap confidence interval? Hint: Draw a sketch.\n",
    "sort the 200 simulations from low to high.  \n",
    "find the index of first 2.5% and last 97.5%.  \n",
    "identify the value associated with the index, the lower bound of the 95% confidence interval would be the value at position first 2.5%, the upper bound of the 95% confidence interval would be the value at position last 97.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703e796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (realestate_env)",
   "language": "python",
   "name": "realestate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
